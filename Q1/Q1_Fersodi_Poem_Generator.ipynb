{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3M-AWEThjhfQ"
      },
      "source": [
        "# Sequence to Sequence LSTM\n",
        "\n",
        "Acknowledgement : with the help of https://github.com/bentrevett/pytorch-seq2seq \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall --yes torchtext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQk4vbDntp0l",
        "outputId": "218731d6-1788-428c-e967-e12ebc21b8b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torchtext 0.14.1\n",
            "Uninstalling torchtext-0.14.1:\n",
            "  Successfully uninstalled torchtext-0.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.10.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8x3EGd6tyY-",
        "outputId": "61f61b56-d8fd-46c2-ae98-05ab7e90617d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.10.0\n",
            "  Downloading torchtext-0.10.0-cp38-cp38-manylinux1_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchtext==0.10.0) (2.25.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchtext==0.10.0) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torchtext==0.10.0) (4.64.1)\n",
            "Collecting torch==1.9.0\n",
            "  Downloading torch-1.9.0-cp38-cp38-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m831.4/831.4 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.9.0->torchtext==0.10.0) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.10.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.10.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.10.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.10.0) (4.0.0)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1+cu116\n",
            "    Uninstalling torch-1.13.1+cu116:\n",
            "      Successfully uninstalled torch-1.13.1+cu116\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.14.1+cu116 requires torch==1.13.1, but you have torch 1.9.0 which is incompatible.\n",
            "torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.9.0 torchtext-0.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2sPrPYmjhfT",
        "outputId": "f72721b4-0d24-4503-ae65-488b740e65ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfor dirname, _, filenames in os.walk('/kaggle/input'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import time, random, math, string\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "#from torchtext.legacy.datasets import Multi30k\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "\n",
        "from torchtext.legacy import data\n",
        "from torchtext.legacy import datasets\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "import collections\n",
        "import pandas as pd\n",
        "import os\n",
        "\"\"\"\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\"\"\"\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext"
      ],
      "metadata": {
        "id": "ehXWndRwpuzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S-2GXGujhfU"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this notebook, we will start simple model to understand the general concepts by implementing the model from the [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215) paper.\n",
        "\n",
        "The most common sequence-to-sequence (seq2seq) models are encoder-decoder models, which (commonly) use a recurrent neural network (RNN) to encode the source (input) sentence into a single vector (as an abstract representation of the entrie input sentence).\n",
        "\n",
        "This vector is then decoded by a second RNN which learns to output the target(output) sentence by generating it one word at a time.\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/raw/3a8dc5515ff28cb059532439c5687126dd30015f/assets/seq2seq1.png)\n",
        "Above image shows an example translation. The input sentence \"guten morgen\", is input into the encoder (green) one word at a time. We also append a start of sequence(<sos\\>) and end of sequence(<eos\\>) token to the start and end of sentence, respectively. At each time-step, the input to the encoder RNN is both the current word $x_t$ as well as the hidden state from the previous time-step $h_{t-1}$ You can think of the hidden state as a vector representation of the sentence so far. The RNN can be represented as a function of both $x_t$ and $h_{t-1}$ :\n",
        "$$\n",
        "h_t = EncoderRNN(x_t, h_{t-1}) \\tag{1}\n",
        "$$\n",
        "Here, we have $X={x_1, x_2, \\cdots, x_T}$ where $x_1$ = <sos\\> $x_2$ = guten, etc. The initial hidden states $h_0$ is usually either initialized to zeros or a learned parameter.\n",
        "\n",
        "Once the final word $x_T$ has been passed into RNN, we use the final hidden state $h_T$ as the context vector i.e. $h_T = z$.\n",
        " \n",
        "With our context vector $z$, we can start decoding it to get the target sentence, \"good morning\". Again we append start and end of sequence tokens to the target sentence. At each time-step, the input to the decoder RNN (blue) is the current word, $y_t$, as well as the hidden state from the previous time-step $s_{t-1}$, where the initail decoder hidden state $s_0 = z = h_T$ i.e. the initial hidden state is the final encoder hidden state. similar to the encoder, we can represent the decoder as:\n",
        "$$\n",
        "s_t = DecoderRNN(y_t, s_{t-1}) \\tag{2}\n",
        "$$\n",
        "In the decoder, we need to go from the hidden state to an actual word, therefore at each time-step we use $s_t$ to predict (by passing it through a Linear layer, shown in purple) what we think is the next word in the sequence $\\hat{y}_t$.\n",
        "$$\n",
        "\\hat{y}_t = f(s_t) \\tag{3}\n",
        "$$\n",
        "The word in the encoder are always generated one after another, with one per time-step. We always use the <sos\\> for the first input to the decoder $y_1$, but for subsequent inputs $y_{t > 1}$, we will sometimes use the actual, ground truth next word in the sequence, $y_t$ and sometimes use the word predicted by our decoder $\\hat{y}_{t-1}$. This is called teacher forcing.\n",
        "\n",
        "When training/testing our model, we always know how many words are in our target sentence, so we stop generating words once we hit that many. During inference (i.e. real world usage) it is common to keep generating words until the model outputs an <eos\\> token or after a certain amount of words have been generated.\n",
        "\n",
        "Once we get our prediction $\\hat{Y} = {\\hat{y_1},\\hat{y_2},\\cdots, \\hat{y_T}}$, we compare it against our actual target sentence $Y = {y_1, y_2, \\cdots y_T}$, to calculate our loss. We then use this loss to update all of the parameters in our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd71o5T0jhfV"
      },
      "source": [
        "## Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "q-siaujsjhfW"
      },
      "outputs": [],
      "source": [
        "tokenizer = lambda x: str(x).translate(str.maketrans('', '', string.punctuation)).strip().split() \n",
        "reverse_tokenizer = lambda x: tokenizer(x)[::-1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "minlim, maxlim = 7, 15"
      ],
      "metadata": {
        "id": "WorwkvzN3x5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!wget 'https://drive.google.com/uc?export=download&id=1qmAPy--y_6gP76PrjM0BCNeCIv3lSDwZ&confirm=T' -O dataset.zip\n",
        "!rm -r dataset\n",
        "!unzip dataset.zip\n",
        "!rm dataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlRawbnq6dZe",
        "outputId": "a2e55522-c054-4c8c-bf5b-f5b0d6a47dbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-05 15:55:32--  https://drive.google.com/uc?export=download&id=1qmAPy--y_6gP76PrjM0BCNeCIv3lSDwZ&confirm=T\n",
            "Resolving drive.google.com (drive.google.com)... 142.250.4.139, 142.250.4.138, 142.250.4.100, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.250.4.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0k-5g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/2he6f1q71lk1nbd0ssalr18hm2k16can/1675612500000/11525140254639593309/*/1qmAPy--y_6gP76PrjM0BCNeCIv3lSDwZ?e=download&uuid=9deac040-1227-4138-ad36-69355a017a7f [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-02-05 15:55:34--  https://doc-0k-5g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/2he6f1q71lk1nbd0ssalr18hm2k16can/1675612500000/11525140254639593309/*/1qmAPy--y_6gP76PrjM0BCNeCIv3lSDwZ?e=download&uuid=9deac040-1227-4138-ad36-69355a017a7f\n",
            "Resolving doc-0k-5g-docs.googleusercontent.com (doc-0k-5g-docs.googleusercontent.com)... 142.251.12.132, 2404:6800:4003:c11::84\n",
            "Connecting to doc-0k-5g-docs.googleusercontent.com (doc-0k-5g-docs.googleusercontent.com)|142.251.12.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2764024 (2.6M) [application/zip]\n",
            "Saving to: ‘dataset.zip’\n",
            "\n",
            "dataset.zip         100%[===================>]   2.64M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-02-05 15:55:35 (257 MB/s) - ‘dataset.zip’ saved [2764024/2764024]\n",
            "\n",
            "rm: cannot remove 'dataset': No such file or directory\n",
            "Archive:  dataset.zip\n",
            "   creating: dataset/\n",
            "  inflating: dataset/valid.csv       \n",
            "  inflating: dataset/train.csv       \n",
            "  inflating: dataset/test.csv        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Text-Mining/Persian-Wikipedia-Corpus.git\n",
        "!unzip Persian-Wikipedia-Corpus/models/glove/vectors.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5o_W0jqYjBOO",
        "outputId": "ee9ae7d8-e9f4-45f0-a012-3c779a3dc771"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Persian-Wikipedia-Corpus'...\n",
            "remote: Enumerating objects: 47, done.\u001b[K\n",
            "remote: Total 47 (delta 0), reused 0 (delta 0), pack-reused 47\u001b[K\n",
            "Unpacking objects: 100% (47/47), 758.15 MiB | 12.10 MiB/s, done.\n",
            "Updating files: 100% (14/14), done.\n",
            "Archive:  Persian-Wikipedia-Corpus/models/glove/vectors.zip\n",
            "  inflating: vectors.txt             \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aIGi2ndLqLC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab,embeddings = [],[]\n",
        "with open('vectors.txt','rt') as fi:\n",
        "    full_content = fi.read().strip().split('\\n')\n",
        "for i in range(len(full_content)):\n",
        "    i_word = full_content[i].split(' ')[0]\n",
        "    i_embeddings = [float(val) for val in full_content[i].split(' ')[1:]]\n",
        "    vocab.append(i_word)\n",
        "    embeddings.append(i_embeddings)"
      ],
      "metadata": {
        "id": "PO66fi-Mn5Yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "vocab_npa = np.array(vocab)\n",
        "embs_npa = np.array(embeddings)"
      ],
      "metadata": {
        "id": "H7q0GB8ooMXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#insert '<pad>' and '<unk>' tokens at start of vocab_npa.\n",
        "vocab_npa = np.insert(vocab_npa, 0, '<unk>')\n",
        "vocab_npa = np.insert(vocab_npa, 1, '<pad>')\n",
        "vocab_npa = np.insert(vocab_npa, 2, '<sos>')\n",
        "vocab_npa = np.insert(vocab_npa, 3, '<eos>')\n",
        "print(vocab_npa[:10])\n",
        "\n",
        "mean = np.mean(embs_npa,axis=0,keepdims=True)\n",
        "variance = np.var(embs_npa,axis=0,keepdims=True)\n",
        "\n",
        "pad_emb_npa = np.zeros((1,embs_npa.shape[1]))   #embedding for '<pad>' token.\n",
        "unk_emb_npa = mean + (2*np.random.rand(variance.shape[0],variance.shape[1])-1)*variance#np.mean(embs_npa,axis=0,keepdims=True)    #embedding for '<unk>' token.\n",
        "sos_emb_npa = mean + (2*np.random.rand(variance.shape[0],variance.shape[1])-1)*variance\n",
        "eos_emb_npa = mean + (2*np.random.rand(variance.shape[0],variance.shape[1])-1)*variance\n",
        "\n",
        "\n",
        "#insert embeddings for pad and unk tokens at top of embs_npa.\n",
        "embs_npa = np.vstack((unk_emb_npa,pad_emb_npa,sos_emb_npa,eos_emb_npa,embs_npa))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e_Q-I5ioVGO",
        "outputId": "d83d18a4-a68d-43ea-fb18-3c3294ad3047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<unk>' '<pad>' '<sos>' '<eos>' 'در' 'و' '<NUM>' 'به' 'از' 'که']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "my_embedding_layer = torch.nn.Embedding.from_pretrained(torch.from_numpy(embs_npa).float())\n",
        "\n",
        "assert my_embedding_layer.weight.shape == embs_npa.shape\n",
        "print(my_embedding_layer.weight.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPHmZ_nxohXH",
        "outputId": "36badde3-dc2c-4a9c-ea6b-9edd208beba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([240552, 50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_rAXhER-oizG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SRC = Field(tokenize=reverse_tokenizer, init_token='<sos>', eos_token='<eos>',fix_length = maxlim)\n",
        "TRG = Field(tokenize=tokenizer, init_token='<sos>', eos_token='<eos>', lower=True,fix_length = maxlim)\n"
      ],
      "metadata": {
        "id": "T3nBL6whDcp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fields = {'beyte1': ('src', SRC),'beyte2': ('trg', TRG)}"
      ],
      "metadata": {
        "id": "JF76yvEq7z3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
        "                                        path = 'dataset',\n",
        "                                        train = 'train.csv',\n",
        "                                        validation = 'valid.csv',\n",
        "                                        test = 'test.csv',\n",
        "                                        format = 'csv',\n",
        "                                        fields = fields,\n",
        "                                        skip_header = False\n",
        ")"
      ],
      "metadata": {
        "id": "MgQn4ysa7Tvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sher1\n",
        "# sher2\n",
        "# sher 3\n",
        "# ->\n",
        "# sher1,sher2\n",
        "# sher2,sher3"
      ],
      "metadata": {
        "id": "c-gn6Y-pzpWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8El9g-fUjhfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cff96e1-541b-4b0f-c253-30f5df74f781"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'src': ['خم', 'رویینه', 'و', 'برکشیدند', 'جرس', 'گاودم', 'نالهٔ', 'و', 'آمد', 'خروش'], 'trg': ['سوی', 'شهر', 'ایران', 'نهادند', 'روی', 'سپاهی', 'بران', 'گونه', 'با', 'رنگ', 'و', 'بوی']}\n"
          ]
        }
      ],
      "source": [
        "print(vars(train_data.examples[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvbAlCPajhfY",
        "outputId": "748032e4-4365-47bd-d901-6818e83cdf11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique tokens in source (de) vocabulary: 8543\n",
            "Unique tokens in target (en) vocabulary: 8577\n"
          ]
        }
      ],
      "source": [
        "SRC.build_vocab(train_data, min_freq=2)\n",
        "TRG.build_vocab(train_data, min_freq=2)\n",
        "\n",
        "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
        "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4unR5pljhfY"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vocab,embeddings = [],[]\n",
        "with open('vectors.txt','rt') as fi:\n",
        "    full_content = fi.read().strip().split('\\n')\n",
        "for i in range(len(full_content)):\n",
        "    i_word = full_content[i].split(' ')[0]\n",
        "    i_embeddings = [float(val) for val in full_content[i].split(' ')[1:]]\n",
        "    vocab.append(i_word)\n",
        "    embeddings.append(i_embeddings)\n",
        "\n",
        "#convert to numpy\n",
        "import numpy as np\n",
        "vocab_npa = np.array(vocab)\n",
        "embs_npa = np.array(embeddings)\n",
        "\n",
        "#insert '<pad>' and '<unk>' tokens at start of vocab_npa.\n",
        "vocab_npa = np.insert(vocab_npa, 0, '<unk>')\n",
        "vocab_npa = np.insert(vocab_npa, 1, '<pad>')\n",
        "vocab_npa = np.insert(vocab_npa, 2, '<sos>')\n",
        "vocab_npa = np.insert(vocab_npa, 3, '<eos>')\n",
        "print(vocab_npa[:10])\n",
        "\n",
        "mean = np.mean(embs_npa,axis=0,keepdims=True)\n",
        "variance = np.var(embs_npa,axis=0,keepdims=True)\n",
        "\n",
        "pad_emb_npa = np.zeros((1,embs_npa.shape[1]))   #embedding for '<pad>' token.\n",
        "unk_emb_npa = mean + (2*np.random.rand(variance.shape[0],variance.shape[1])-1)*variance\n",
        "sos_emb_npa = mean + (2*np.random.rand(variance.shape[0],variance.shape[1])-1)*variance\n",
        "eos_emb_npa = mean + (2*np.random.rand(variance.shape[0],variance.shape[1])-1)*variance\n",
        "\n",
        "\n",
        "#insert embeddings for pad and unk tokens at top of embs_npa.\n",
        "embs_npa = np.vstack((unk_emb_npa,pad_emb_npa,sos_emb_npa,eos_emb_npa,embs_npa))\n",
        "print(embs_npa.shape)\n",
        "\n",
        "\n",
        "\n",
        "#convert to torch\n",
        "embs_torch = torch.tensor(embs_npa,dtype=torch.float).to(device)\n",
        "\n",
        "#mean & variance\n",
        "\n",
        "\n",
        "# create vectors\n",
        "indices = np.arange(len(embs_npa))\n",
        "stoi_dict = {vocab_npa[i]:i for i in indices}\n",
        "emb_dict = {vocab_npa[i]:embs_npa[i] for i in range(len(vocab_npa))}\n",
        "SRC.vocab.set_vectors(stoi=stoi_dict,vectors=embs_torch,dim=50)\n",
        "TRG.vocab.set_vectors(stoi=stoi_dict,vectors=embs_torch,dim=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcikuFrgKHjR",
        "outputId": "8b72a862-5ad0-4e6b-db73-8f91f5d283f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<unk>' '<pad>' '<sos>' '<eos>' 'در' 'و' '<NUM>' 'به' 'از' 'که']\n",
            "(240552, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "BATHC_SIZE = 128\n",
        "# We use a BucketIterator instead of the standard Iterator as it create batches in such a way that it minimizes the amount \n",
        "# of padding in both the source and target sentences.\n",
        "train_iter, valid_iter, test_iter = BucketIterator.splits((train_data, valid_data, test_data),\n",
        "                                                          batch_size=BATHC_SIZE, device=device,sort_within_batch = True,\n",
        "                                                          sort_key = lambda x: len(x.src))"
      ],
      "metadata": {
        "id": "blcZrCfUjZZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, batch in enumerate(train_iter):\n",
        "    a = batch\n",
        "    break\n"
      ],
      "metadata": {
        "id": "8P9YVDp_2Xlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a.trg.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9555ZBxVqnn",
        "outputId": "90ae478a-70ea-4010-d487-9e2c6eb10441"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([15, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMdMm9UwjhfY"
      },
      "source": [
        "## Building the Seq2Seq Model\n",
        "\n",
        "We will build our model in three parts: The encoder, the decoder, and a seq2seq model that encapsulates the encoder and decoder. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lg44LbCQjhfY"
      },
      "source": [
        "### Encoder \n",
        "First, the encoder, a 2 layer LSTM. The paper we are implementing uses a 4-layer LSTM, but in the interest of training time we cut down to 2-layers. The concept of multi-layer RNN is easy to expand from 2 to 4 layers.\n",
        "\n",
        "For a multi-layer RNN, the input sentence, $X$, goes into the first (bottom) layer of the RNN and hiddne states, $H=\\{h_1, h_2, \\cdots,h_T\\}$ output by this layer are used as inputs to the RNN in the layer above. Thus representing each layer with a superscript, the hidden states in the first layer are given by :\n",
        "$$\n",
        "h_t^1 = EncoderRNN^1(x_t, h_{t-1}^1) \\tag{4}\n",
        "$$\n",
        "The hidden states in the second layer are given by:\n",
        "$$\n",
        "h_t^2 = EncoderRNN^2(h_t^1, h_{t-1}^2) \\tag{5} \n",
        "$$\n",
        "Using a multi-layer RNN also means we'll also need an initial hidden state as input per layer, $h_0^l$, and we will also output a context vector per layer $z^l$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlO5KS81jhfZ"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout,SRC_Field):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        #self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.embedding = nn.Embedding.from_pretrained(SRC_Field.vocab.vectors, freeze=False)\n",
        "\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers, dropout=dropout)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, src):\n",
        "        # src : [sen_len, batch_size]\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        # embedded : [sen_len, batch_size, emb_dim]\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        # outputs = [sen_len, batch_size, hid_dim * n_directions]\n",
        "        # hidden = [n_layers * n_direction, batch_size, hid_dim]\n",
        "        # cell = [n_layers * n_direction, batch_size, hid_dim]\n",
        "        return hidden, cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1P-ujzzYjhfZ"
      },
      "source": [
        "### Decoder\n",
        "\n",
        "Next, we will build our decoder. which also be a 2-layer LSTM.\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/raw/3a8dc5515ff28cb059532439c5687126dd30015f/assets/seq2seq3.png)\n",
        "\n",
        "We can use the following equations to explain the decoder model.\n",
        "$$\n",
        "(s_t^1, c_t^1) = DecoderLSTM^1(y_t, (s_{t-1}^1, c_{t-1}^1))\n",
        "$$\n",
        "$$\n",
        "(s_t^2, c_t^2) = DecoderLSTM^2(s_t^1, (s_{t-1}^2, c_{t-1}^2))\n",
        "$$\n",
        "Remember that the initial hidden and cell states to our decoder are our context vectors, which are the final hidden and cell of our encoder from the same layer. i.e. $(s_0^l, c_0^l) = z^l = (h_T^l, c_T^l)$.\n",
        "\n",
        "We then pass the hidden state from the top layer of the RNN, $s_t^2$ through a linear layer $f$, to make a prediction of what the next token in the target (output) sequence should be $\\hat{y}_{t+1}$\n",
        "$$\n",
        "\\hat{y}_{t+1} = f(s_t^2) \n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytA9CO2UjhfZ"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout,TRG_Field):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.output_dim = output_dim\n",
        "        self.emb_dim = emb_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        #self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.embedding = nn.Embedding.from_pretrained(TRG_Field.vocab.vectors, freeze=False)\n",
        "\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=self.n_layers, dropout=dropout)\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, cell):\n",
        "        \n",
        "        # input = [batch_size]\n",
        "        # hidden = [n_layers * n_dir, batch_size, hid_dim]\n",
        "        # cell = [n_layers * n_dir, batch_size, hid_dim]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        # input : [1, ,batch_size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        # embedded = [1, batch_size, emb_dim]\n",
        "        \n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        # output = [seq_len, batch_size, hid_dim * n_dir]\n",
        "        # hidden = [n_layers * n_dir, batch_size, hid_dim]\n",
        "        # cell = [n_layers * n_dir, batch_size, hid_dim]\n",
        "        \n",
        "        # seq_len and n_dir will always be 1 in the decoder\n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        # prediction = [batch_size, output_dim]\n",
        "        return prediction, hidden, cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYwQKwNFjhfa"
      },
      "source": [
        "### Seq2Seq\n",
        "For the final part of the implementation, we will implement the seq2seq model.\n",
        "\n",
        "- receive the input/source sentence\n",
        "\n",
        "- using the encoder to produce the context vectors\n",
        "\n",
        "- using the decoder to produce the predicted output / target sentence.\n",
        "\n",
        "Our full model will look like this:\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/raw/3a8dc5515ff28cb059532439c5687126dd30015f/assets/seq2seq4.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uD1qmtyIjhfa"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            'hidden dimensions of encoder and decoder must be equal.'\n",
        "        assert encoder.n_layers == decoder.n_layers, \\\n",
        "            'n_layers of encoder and decoder must be equal.'\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio=1):\n",
        "        # src = [sen_len, batch_size]\n",
        "        # trg = [sen_len, batch_size]\n",
        "        # teacher_forcing_ratio : the probability to use the teacher forcing.\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        # tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        hidden, cell = self.encoder(src)\n",
        "        \n",
        "        # first input to the decoder is the <sos> token.\n",
        "        input = trg[0, :]\n",
        "        for t in range(1, trg_len):\n",
        "            # insert input token embedding, previous hidden and previous cell states \n",
        "            # receive output tensor (predictions) and new hidden and cell states.\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            \n",
        "            # replace predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            # decide if we are going to use teacher forcing or not.\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            # get the highest predicted token from our predictions.\n",
        "            top1 = output.argmax(1)\n",
        "            # update input : use ground_truth when teacher_force \n",
        "            input = trg[t] if teacher_force else top1\n",
        "            \n",
        "        return outputs\n",
        "        \n",
        "    def inference(self, src,maxlim=15):\n",
        "        # src = [sen_len, batch_size]\n",
        "        # trg = [sen_len, batch_size]\n",
        "        # teacher_forcing_ratio : the probability to use the teacher forcing.\n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = maxlim\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        # tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        hidden, cell = self.encoder(src)\n",
        "        \n",
        "        # first input to the decoder is the <sos> token.\n",
        "        input = (torch.ones((batch_size),dtype=torch.long)*2).to(device)\n",
        "        for t in range(1, trg_len):\n",
        "            # insert input token embedding, previous hidden and previous cell states \n",
        "            # receive output tensor (predictions) and new hidden and cell states.\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            \n",
        "            # replace predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            # get the highest predicted token from our predictions.\n",
        "            top1 = output.argmax(1)\n",
        "            # update input : use ground_truth when teacher_force \n",
        "            input = top1\n",
        "            \n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SRC.vocab.stoi['<sos>']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPJ-l_XTSJ_O",
        "outputId": "2407124b-8e06-4114-c477-11db58bb2ac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhdf3eRTjhfa"
      },
      "source": [
        "## Training the Seq2Seq model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shQcux5Kjhfa"
      },
      "outputs": [],
      "source": [
        "# First initialize our model.\n",
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 50\n",
        "DEC_EMB_DIM = 50\n",
        "HID_DIM = 128\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT,SRC)\n",
        "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT,TRG)\n",
        "\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0cma22ajhfb",
        "outputId": "1f6ed416-f304-4fc0-b043-e42928b23589"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(8543, 50)\n",
              "    (rnn): LSTM(50, 128, num_layers=2, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(8577, 50)\n",
              "    (rnn): LSTM(50, 128, num_layers=2, dropout=0.5)\n",
              "    (fc_out): Linear(in_features=128, out_features=8577, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "model.apply(init_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfgJzR5djhfb",
        "outputId": "8ad82a88-de9a-4560-9cc3-a8b9764c7429"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 2,410,945 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN4vU4nYjhfb"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlDsxOvzjhfc"
      },
      "source": [
        "Next, we'll define our training loop.\n",
        "\n",
        "First. we'll set the model into \"training mode\" (turn on the dropout & batch normalization), and then iterate through our data iterator.\n",
        "\n",
        "As stated before, our decoder loop starts at 1, not 0. This means the 0th element of our outputs tensor remains all zeros. So our trg & outputs look something like:\n",
        "$$\n",
        "trg = [<sos>, y_1, y_2, y_3, <eos>]\n",
        "$$\n",
        "$$\n",
        "output = [0, \\hat{y}_1,\\hat{y}_2,\\hat{y}_3, <eos>]\n",
        "$$\n",
        "Here, when we calculate the loss, we cut off the first element of each tensor to get:\n",
        "$$\n",
        "trg = [y_1, y_2, y_3, <eos>]\n",
        "$$\n",
        "$$\n",
        "output = [\\hat{y}_1,\\hat{y}_2,\\hat{y}_3, <eos>]\n",
        "$$\n",
        "At each iterator:\n",
        "\n",
        "- get the source and target sentences from the batch, X and Y\n",
        "\n",
        "- zero the gradients calculated from the last batch\n",
        " \n",
        "- feed the source and target into the model to get the output $\\hat{y}$\n",
        "\n",
        "- as the loss function only works on 2d inputs with 1d targets we need to flatten each of them with .view\n",
        "    - we slice off the first column of the output and target tensors as mentioned above.\n",
        "    \n",
        "- calculate the gradients with loss.backward()\n",
        "\n",
        "- clip the gradients to prevent them from exploding (a common issue in RNNs)\n",
        "\n",
        "- update the parameters of our model by doing an optimizer.step()\n",
        "\n",
        "- sum the loss value to a running total.\n",
        "\n",
        "Finally, we return the loss that is average over all batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iA826O_yjhfc"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    for i, batch in enumerate(iterator):\n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        # trg = [sen_len, batch_size]\n",
        "        # output = [trg_len, batch_size, output_dim]\n",
        "        output = model(src, trg)\n",
        "\n",
        "        output_index = output.argmax(2)\n",
        "        acc = (torch.sum(output_index==trg)/torch.numel(output_index)).item()\n",
        "\n",
        "        epoch_acc += acc\n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        # transfrom our output : slice off the first column, and flatten the output into 2 dim.\n",
        "        output = output[1:].view(-1, output_dim) \n",
        "        trg = trg[1:].view(-1)\n",
        "        # trg = [(trg_len-1) * batch_size]\n",
        "        # output = [(trg_len-1) * batch_size, output_dim]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator),epoch_acc/ len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nI7q2aPBjhfc"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        for i, batch in enumerate(iterator):\n",
        "            \n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "            \n",
        "            output = model(src, trg, 0) # turn off teacher forcing.\n",
        "            output_index = output.argmax(2)\n",
        "            acc = (torch.sum(output_index==trg)/torch.numel(output_index)).item()\n",
        "            epoch_acc += acc\n",
        "            # trg = [sen_len, batch_size]\n",
        "            # output = [sen_len, batch_size, output_dim]\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "            \n",
        "            loss = criterion(output, trg)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            \n",
        "    return epoch_loss / len(iterator),epoch_acc/ len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYJ3lqNBjhfc"
      },
      "outputs": [],
      "source": [
        "# a function that used to tell us how long an epoch takes.\n",
        "def epoch_time(start_time, end_time):\n",
        "    \n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time  / 60)\n",
        "    elapsed_secs = int(elapsed_time -  (elapsed_mins * 60))\n",
        "    return  elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXXzaNuAjhfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73c67500-d169-4766-a3ce-4b87cd630be2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time 0m 16s\n",
            "\tTrain Loss: 6.622 | Train PPL: 751.583\n",
            "\tValid Loss: 6.373 | Valid PPL: 586.025\n",
            "Epoch: 02 | Time 0m 16s\n",
            "\tTrain Loss: 6.250 | Train PPL: 518.120\n",
            "\tValid Loss: 6.741 | Valid PPL: 846.631\n",
            "Epoch: 03 | Time 0m 16s\n",
            "\tTrain Loss: 6.076 | Train PPL: 435.368\n",
            "\tValid Loss: 6.600 | Valid PPL: 735.114\n",
            "Epoch: 04 | Time 0m 16s\n",
            "\tTrain Loss: 5.918 | Train PPL: 371.505\n",
            "\tValid Loss: 6.747 | Valid PPL: 851.550\n",
            "Epoch: 05 | Time 0m 16s\n",
            "\tTrain Loss: 5.801 | Train PPL: 330.567\n",
            "\tValid Loss: 6.701 | Valid PPL: 812.980\n",
            "Epoch: 06 | Time 0m 16s\n",
            "\tTrain Loss: 5.719 | Train PPL: 304.652\n",
            "\tValid Loss: 6.779 | Valid PPL: 879.213\n",
            "Epoch: 07 | Time 0m 16s\n",
            "\tTrain Loss: 5.651 | Train PPL: 284.716\n",
            "\tValid Loss: 6.810 | Valid PPL: 907.081\n",
            "Epoch: 08 | Time 0m 16s\n",
            "\tTrain Loss: 5.590 | Train PPL: 267.668\n",
            "\tValid Loss: 6.919 | Valid PPL: 1011.776\n",
            "Epoch: 09 | Time 0m 16s\n",
            "\tTrain Loss: 5.533 | Train PPL: 252.926\n",
            "\tValid Loss: 6.957 | Valid PPL: 1050.118\n",
            "Epoch: 10 | Time 0m 16s\n",
            "\tTrain Loss: 5.475 | Train PPL: 238.671\n",
            "\tValid Loss: 6.966 | Valid PPL: 1059.698\n",
            "Epoch: 11 | Time 0m 16s\n",
            "\tTrain Loss: 5.419 | Train PPL: 225.642\n",
            "\tValid Loss: 7.007 | Valid PPL: 1104.317\n",
            "Epoch: 12 | Time 0m 16s\n",
            "\tTrain Loss: 5.363 | Train PPL: 213.470\n",
            "\tValid Loss: 7.157 | Valid PPL: 1283.541\n",
            "Epoch: 13 | Time 0m 16s\n",
            "\tTrain Loss: 5.312 | Train PPL: 202.705\n",
            "\tValid Loss: 7.555 | Valid PPL: 1910.564\n",
            "Epoch: 14 | Time 0m 16s\n",
            "\tTrain Loss: 5.266 | Train PPL: 193.611\n",
            "\tValid Loss: 7.625 | Valid PPL: 2048.812\n",
            "Epoch: 15 | Time 0m 16s\n",
            "\tTrain Loss: 5.225 | Train PPL: 185.915\n",
            "\tValid Loss: 8.040 | Valid PPL: 3103.320\n",
            "Epoch: 16 | Time 0m 16s\n",
            "\tTrain Loss: 5.186 | Train PPL: 178.770\n",
            "\tValid Loss: 8.125 | Valid PPL: 3378.548\n",
            "Epoch: 17 | Time 0m 16s\n",
            "\tTrain Loss: 5.153 | Train PPL: 172.967\n",
            "\tValid Loss: 8.193 | Valid PPL: 3614.728\n",
            "Epoch: 18 | Time 0m 16s\n",
            "\tTrain Loss: 5.121 | Train PPL: 167.420\n",
            "\tValid Loss: 8.173 | Valid PPL: 3543.609\n",
            "Epoch: 19 | Time 0m 16s\n",
            "\tTrain Loss: 5.091 | Train PPL: 162.503\n",
            "\tValid Loss: 8.223 | Valid PPL: 3727.429\n",
            "Epoch: 20 | Time 0m 16s\n",
            "\tTrain Loss: 5.062 | Train PPL: 157.884\n",
            "\tValid Loss: 8.292 | Valid PPL: 3992.668\n",
            "Epoch: 21 | Time 0m 16s\n",
            "\tTrain Loss: 5.034 | Train PPL: 153.572\n",
            "\tValid Loss: 8.357 | Valid PPL: 4260.148\n",
            "Epoch: 22 | Time 0m 16s\n",
            "\tTrain Loss: 5.008 | Train PPL: 149.536\n",
            "\tValid Loss: 8.398 | Valid PPL: 4436.181\n",
            "Epoch: 23 | Time 0m 16s\n",
            "\tTrain Loss: 4.982 | Train PPL: 145.738\n",
            "\tValid Loss: 8.532 | Valid PPL: 5074.666\n",
            "Epoch: 24 | Time 0m 16s\n",
            "\tTrain Loss: 4.956 | Train PPL: 142.083\n",
            "\tValid Loss: 8.537 | Valid PPL: 5099.349\n",
            "Epoch: 25 | Time 0m 16s\n",
            "\tTrain Loss: 4.932 | Train PPL: 138.590\n",
            "\tValid Loss: 8.634 | Valid PPL: 5617.477\n",
            "Epoch: 26 | Time 0m 16s\n",
            "\tTrain Loss: 4.908 | Train PPL: 135.352\n",
            "\tValid Loss: 8.766 | Valid PPL: 6410.192\n",
            "Epoch: 27 | Time 0m 16s\n",
            "\tTrain Loss: 4.887 | Train PPL: 132.518\n",
            "\tValid Loss: 8.698 | Valid PPL: 5992.330\n",
            "Epoch: 28 | Time 0m 16s\n",
            "\tTrain Loss: 4.864 | Train PPL: 129.504\n",
            "\tValid Loss: 8.812 | Valid PPL: 6715.734\n",
            "Epoch: 29 | Time 0m 16s\n",
            "\tTrain Loss: 4.842 | Train PPL: 126.676\n",
            "\tValid Loss: 8.885 | Valid PPL: 7221.264\n",
            "Epoch: 30 | Time 0m 16s\n",
            "\tTrain Loss: 4.824 | Train PPL: 124.418\n",
            "\tValid Loss: 8.975 | Valid PPL: 7901.230\n"
          ]
        }
      ],
      "source": [
        "history = collections.defaultdict(list)\n",
        "N_EPOCHS = 30\n",
        "save_every = 3\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss,train_acc = train(model, train_iter, optimizer, criterion, CLIP)\n",
        "    valid_loss,valid_acc = evaluate(model, valid_iter, criterion)\n",
        "\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(valid_loss)\n",
        "    history['val_acc'].append(valid_acc)\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'Seq2SeqModel.pt')\n",
        "    if epoch%save_every == 0:\n",
        "      torch.save(model.state_dict(), f'Seq2SeqModel{epoch:02d}.pt')\n",
        "      data = str(history)\n",
        "      with open(f'losses{epoch:02d}.txt','wt') as f:\n",
        "        f.write(data)\n",
        "\n",
        "      \n",
        "    print(f\"Epoch: {epoch+1:02} | Time {epoch_mins}m {epoch_secs}s\")\n",
        "    print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\")\n",
        "    print(f\"\\tValid Loss: {valid_loss:.3f} | Valid PPL: {math.exp(valid_loss):7.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "model.train()\n",
        "\n",
        "epoch_loss = 0\n",
        "clip = 1\n",
        "\n",
        "for i, batch in enumerate(train_iter):\n",
        "    src = batch.src\n",
        "    trg = batch.trg\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    # trg = [sen_len, batch_size]\n",
        "    # output = [trg_len, batch_size, output_dim]\n",
        "    output = model(src, trg)\n",
        "    output_index = output.argmax(2)\n",
        "    acc = (torch.sum(output_index==trg)/torch.numel(output_index)).item()\n",
        "    output_dim = output.shape[-1]\n",
        "    \n",
        "    # transfrom our output : slice off the first column, and flatten the output into 2 dim.\n",
        "    output = output[1:].view(-1, output_dim) \n",
        "    trg = trg[1:].view(-1)\n",
        "    # trg = [(trg_len-1) * batch_size]\n",
        "    # output = [(trg_len-1) * batch_size, output_dim]\n",
        "    \n",
        "    loss = criterion(output, trg)\n",
        "    \n",
        "    loss.backward()\n",
        "    \n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    \n",
        "    optimizer.step()\n",
        "    \n",
        "    epoch_loss += loss.item()\n",
        "  \"\"\""
      ],
      "metadata": {
        "id": "GIxZLk7WoF3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, batch in enumerate(test_iter):\n",
        "    src = batch.src\n",
        "    break\n"
      ],
      "metadata": {
        "id": "I0b6OpFTXyJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.inference(src,maxlim=15)\n",
        "out = output.argmax(2)\n",
        "out_text = [TRG.vocab.itos[idx] for idx in out[:,27] ]\n",
        "print(out_text)"
      ],
      "metadata": {
        "id": "hwoxE3tfYQDo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5602226-e529-49a3-faa0-8c79069e457d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<unk>', 'چو', 'بشنید', 'و', '<unk>', 'و', '<unk>', 'که', 'از', '<unk>', 'و', '<unk>', 'و', 'سپاه', '<eos>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCwhBc3Djhfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "170ae275-9ce1-42d2-f6f0-c848258cc9b0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-0474c1a390d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test Loss : {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-0474c1a390d5>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeq2Seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Seq2SeqModel.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'encoder' is not defined"
          ]
        }
      ],
      "source": [
        "def test():\n",
        "    best_model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "    best_model.load_state_dict(torch.load('Seq2SeqModel.pt'))\n",
        "    \n",
        "    test_loss = evaluate(model, test_iter, criterion)\n",
        "    \n",
        "    print(f\"Test Loss : {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f}\")\n",
        "    \n",
        "test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vADdYzN0jhfd"
      },
      "source": [
        "## Summary \n",
        "\n",
        "Through the model is tranditional Seq2Seq model as the author mentioned before. As a beginner of pytorch and deeplearning, there is also many useful tricks worth learning. As a notebook I list them below:\n",
        "\n",
        "### Data Preparing Part\n",
        "\n",
        "- In the original paper, they find it beneficial to reverse the order of the input which they believe \"introduces many short term dependencies in the data that make the optimization problem much easier\". \n",
        "\n",
        "it means when we want to get the context vector, it is beneficial to reverse the order of the input.\n",
        "\n",
        "- When build the vocabulary we can use min_freq parameter to remove the rare words in the corpus.\n",
        "\n",
        "e.g. SRC.build_vocab(train_data, min_freq=2)\n",
        "\n",
        "### Seq2Seq Model Part\n",
        "\n",
        "- In this notebook, we can learn how to build a deeplearning pipeline (seperate our model into different parts.) and combine them together.\n",
        "\n",
        "- It is a traditional Seq2Seq model, encoder is used to get the context vectors represented by the hidden and cell state generated by the last layer of LSTM. While decoder initialize its $h_0, c_0$ according to the encoder output. And each time-step $t$, generate the $t+1$ word in the target sentence. update the input according to the previous word and the teacher_force rate.\n",
        "\n",
        "- This is the first time I have ever seen the teacher_force rate. It is just a simple but useful way to restinct the changing of our model.\n",
        "\n",
        "### Train Part\n",
        "\n",
        "- Clip : as mentioned before, before we use optimizer.step(), we should use  torch.nn.utils.clip_grad_norm_(model.parameters(), clip) to avoiding gradients exploding ! Here we set clip=1. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRU"
      ],
      "metadata": {
        "id": "4GHwalx3p0LD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderGRU(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout,SRC_Field):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        #self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.embedding = nn.Embedding.from_pretrained(SRC_Field.vocab.vectors, freeze=False)\n",
        "\n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim, num_layers=n_layers, dropout=dropout,bidirectional=True)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, src):\n",
        "        # src : [sen_len, batch_size]\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        # embedded : [sen_len, batch_size, emb_dim]\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "        # outputs = [sen_len, batch_size, hid_dim * n_directions]\n",
        "        # hidden = [n_layers * n_direction, batch_size, hid_dim]\n",
        "        # cell = [n_layers * n_direction, batch_size, hid_dim]\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "tZWm9c-Qp118"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderGRU(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout,TRG_Field):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.output_dim = output_dim\n",
        "        self.emb_dim = emb_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        #self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.embedding = nn.Embedding.from_pretrained(TRG_Field.vocab.vectors, freeze=False)\n",
        "\n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim, num_layers=self.n_layers, dropout=dropout,bidirectional=True)\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden):\n",
        "        \n",
        "        # input = [batch_size]\n",
        "        # hidden = [n_layers * n_dir, batch_size, hid_dim]\n",
        "        # cell = [n_layers * n_dir, batch_size, hid_dim]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        # input : [1, ,batch_size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        # embedded = [1, batch_size, emb_dim]\n",
        "        \n",
        "        output, hidden= self.rnn(embedded, hidden)\n",
        "        # output = [seq_len, batch_size, hid_dim * n_dir]\n",
        "        # hidden = [n_layers * n_dir, batch_size, hid_dim]\n",
        "        # cell = [n_layers * n_dir, batch_size, hid_dim]\n",
        "        \n",
        "        # seq_len and n_dir will always be 1 in the decoder\n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        # prediction = [batch_size, output_dim]\n",
        "        return prediction, hidden"
      ],
      "metadata": {
        "id": "k22Qq5d6qLt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2SeqGRU(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            'hidden dimensions of encoder and decoder must be equal.'\n",
        "        assert encoder.n_layers == decoder.n_layers, \\\n",
        "            'n_layers of encoder and decoder must be equal.'\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio=1):\n",
        "        # src = [sen_len, batch_size]\n",
        "        # trg = [sen_len, batch_size]\n",
        "        # teacher_forcing_ratio : the probability to use the teacher forcing.\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        # tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        hidden = self.encoder(src)\n",
        "        \n",
        "        # first input to the decoder is the <sos> token.\n",
        "        input = trg[0, :]\n",
        "        for t in range(1, trg_len):\n",
        "            # insert input token embedding, previous hidden and previous cell states \n",
        "            # receive output tensor (predictions) and new hidden and cell states.\n",
        "            output, hidden = self.decoder(input, hidden)\n",
        "            \n",
        "            # replace predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            # decide if we are going to use teacher forcing or not.\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            # get the highest predicted token from our predictions.\n",
        "            top1 = output.argmax(1)\n",
        "            # update input : use ground_truth when teacher_force \n",
        "            input = trg[t] if teacher_force else top1\n",
        "            \n",
        "        return outputs\n",
        "        \n",
        "    def inference(self, src,maxlim=15):\n",
        "        # src = [sen_len, batch_size]\n",
        "        # trg = [sen_len, batch_size]\n",
        "        # teacher_forcing_ratio : the probability to use the teacher forcing.\n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = maxlim\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        # tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        hidden = self.encoder(src)\n",
        "        \n",
        "        # first input to the decoder is the <sos> token.\n",
        "        input = (torch.ones((batch_size),dtype=torch.long)*2).to(device)\n",
        "        for t in range(1, trg_len):\n",
        "            # insert input token embedding, previous hidden and previous cell states \n",
        "            # receive output tensor (predictions) and new hidden and cell states.\n",
        "            output, hidden = self.decoder(input, hidden)\n",
        "            \n",
        "            # replace predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            # get the highest predicted token from our predictions.\n",
        "            top1 = output.argmax(1)\n",
        "            # update input : use ground_truth when teacher_force \n",
        "            input = top1\n",
        "            \n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "5Ktwsqp-sINM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First initialize our model.\n",
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 50\n",
        "DEC_EMB_DIM = 50\n",
        "HID_DIM = 128\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "encodergru = EncoderGRU(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT,SRC)\n",
        "decodergru = DecoderGRU(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT,TRG)\n",
        "\n",
        "modelgru = Seq2SeqGRU(encodergru, decodergru, device).to(device)"
      ],
      "metadata": {
        "id": "vM4GVA1ysvZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(modelgru.parameters())\n",
        "\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)\n"
      ],
      "metadata": {
        "id": "N7BXTOwlvMD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = collections.defaultdict(list)\n",
        "N_EPOCHS = 10\n",
        "save_every = 3\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss,train_acc = train(modelgru, train_iter, optimizer, criterion, CLIP)\n",
        "    valid_loss,valid_acc = evaluate(modelgru, valid_iter, criterion)\n",
        "\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(valid_loss)\n",
        "    history['val_acc'].append(valid_acc)\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(modelgru.state_dict(), 'Seq2SeqModelGRU.pt')\n",
        "    if epoch%save_every == 0:\n",
        "      torch.save(modelgru.state_dict(), f'Seq2SeqModelGRU{epoch:02d}.pt')\n",
        "      data = str(history)\n",
        "      with open(f'lossesGRU{epoch:02d}.txt','wt') as f:\n",
        "        f.write(data)\n",
        "\n",
        "      \n",
        "    print(f\"Epoch: {epoch+1:02} | Time {epoch_mins}m {epoch_secs}s\")\n",
        "    print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\")\n",
        "    print(f\"\\tValid Loss: {valid_loss:.3f} | Valid PPL: {math.exp(valid_loss):7.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "zv5RWS9VsiYX",
        "outputId": "15784b3e-fcf4-4339-e11d-59ca96688b50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-bd1914652ac5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelgru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelgru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-7ec9610756f0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Function AddmmBackward returned an invalid gradient at index 1 - got [128, 128] but expected shape compatible with [128, 256]"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}